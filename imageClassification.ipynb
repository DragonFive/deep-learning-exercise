{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下载模型文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Downloading vgg_16_2016_08_28.tar.gz 100.0%\n",
      "Successfully downloaded vgg_16_2016_08_28.tar.gz 513324920 bytes.\n"
     ]
    }
   ],
   "source": [
    "from datasets import dataset_utils\n",
    "import tensorflow as tf\n",
    "\n",
    "# url = \"http://download.tensorflow.org/models/mobilenet_v1_0.50_128_2017_06_14.tar.gz\"\n",
    "url = \"http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz\"\n",
    "# 指定保存路径\n",
    "checkpoints_dir = '/notebooks/test/checkpoints'\n",
    "\n",
    "if not tf.gfile.Exists(checkpoints_dir):\n",
    "    tf.gfile.MakeDirs(checkpoints_dir)\n",
    "\n",
    "dataset_utils.download_and_uncompress_tarball(url, checkpoints_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在python中import datasets需要先下载models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mmobilenet_v1_0.50_128_2017_06_14\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls /notebooks/test/checkpoints/mobilenet_v1_0.50_128_2017_06_14/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "sys.path.append(\"/notebooks/test/models/slim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面使用预训练好的模型进行图像分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No op named SSTableReaderV2 in defined operations.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-7dab2060b680>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Load weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m#init_fn(sess)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/notebooks/test/checkpoints/mobilenet_v1_1.0_224_2017_06_14/mobilenet_v1_1.0_224.ckpt.meta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/notebooks/test/checkpoints/mobilenet_v1_1.0_224_2017_06_14/mobilenet_v1_1.0_224.ckpt.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1684\u001b[0m                                       \u001b[0mclear_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m                                       \u001b[0mimport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1686\u001b[0;31m                                       **kwargs)\n\u001b[0m\u001b[1;32m   1687\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHasField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saver_def\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimport_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.pyc\u001b[0m in \u001b[0;36mimport_scoped_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, graph, import_scope, input_map, unbound_inputs_col_name, restore_collections_predicate)\u001b[0m\n\u001b[1;32m    502\u001b[0m     importer.import_graph_def(\n\u001b[1;32m    503\u001b[0m         \u001b[0minput_graph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimport_scope\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m         producer_op_list=producer_op_list)\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     scope_to_prepend_to_names = \"/\".join(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.pyc\u001b[0m in \u001b[0;36mimport_graph_def\u001b[0;34m(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;31m# Set any default attr values that aren't present.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mop_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No op named %s in defined operations.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m       \u001b[0mop_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mattr_def\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No op named SSTableReaderV2 in defined operations."
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import urllib2\n",
    "\n",
    "from datasets import imagenet\n",
    "from nets import vgg\n",
    "from nets import mobilenet_v1\n",
    "\n",
    "from preprocessing import vgg_preprocessing\n",
    "\n",
    "checkpoints_dir = '/notebooks/test/checkpoints/'\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "# We need default size of image for a particular network.\n",
    "# The network was trained on images of that size -- so we\n",
    "# resize input image later in the code.\n",
    "image_size = vgg.vgg_16.default_image_size\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    url = (\"https://upload.wikimedia.org/wikipedia/commons/d/d9/First_Student_IC_school_bus_202076.jpg\")\n",
    "\n",
    "    \n",
    "    # Open specified url and load image as a string\n",
    "    image_string = urllib2.urlopen(url).read()\n",
    "    \n",
    "    # Decode string into matrix with intensity values\n",
    "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    \n",
    "    # Resize the input image, preserving the aspect ratio\n",
    "    # and make a central crop of the resulted image.\n",
    "    # The crop will be of the size of the default image size of\n",
    "    # the network.\n",
    "    processed_image = vgg_preprocessing.preprocess_image(image,\n",
    "                                                         image_size,\n",
    "                                                         image_size,\n",
    "                                                         is_training=False)\n",
    "    \n",
    "    # Networks accept images in batches.\n",
    "    # The first dimension usually represents the batch size.\n",
    "    # In our case the batch size is one.\n",
    "    processed_images  = tf.expand_dims(processed_image, 0)\n",
    "    \n",
    "    # Create the model, use the default arg scope to configure\n",
    "    # the batch norm parameters. arg_scope is a very conveniet\n",
    "    # feature of slim library -- you can define default\n",
    "    # parameters for layers -- like stride, padding etc.\n",
    "    with slim.arg_scope(vgg.vgg_arg_scope()):\n",
    "        logits, _ = vgg.vgg_16(processed_images,\n",
    "                               num_classes=1000,\n",
    "                               is_training=False)\n",
    "    \n",
    "    # In order to get probabilities we apply softmax on the output.\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Create a function that reads the network weights\n",
    "    # from the checkpoint file that you downloaded.\n",
    "    # We will run it in session later.\n",
    "#     init_fn = slim.assign_from_checkpoint_fn(\n",
    "#         os.path.join(checkpoints_dir, 'mobilenet_v1_0.50_128.ckpt.meta'),\n",
    "#         slim.get_model_variables('mobilenet_v1_0.50_128'))\n",
    "    \n",
    "    #with tf.Session() as sess:\n",
    "    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:    \n",
    "        # Load weights\n",
    "        #init_fn(sess)\n",
    "        saver = tf.train.import_meta_graph(\"/notebooks/test/checkpoints/mobilenet_v1_1.0_224_2017_06_14/mobilenet_v1_1.0_224.ckpt.meta\")\n",
    "        saver.restore(sess, \"/notebooks/test/checkpoints/mobilenet_v1_1.0_224_2017_06_14/mobilenet_v1_1.0_224.ckpt.ckpt\")\n",
    "\n",
    "        # We want to get predictions, image as numpy matrix\n",
    "        # and resized and cropped piece that is actually\n",
    "        # being fed to the network.\n",
    "        np_image, network_input, probabilities = sess.run([image,\n",
    "                                                           processed_image,\n",
    "                                                           probabilities])\n",
    "        probabilities = probabilities[0, 0:]\n",
    "        sorted_inds = [i[0] for i in sorted(enumerate(-probabilities),\n",
    "                                            key=lambda x:x[1])]\n",
    "    \n",
    "    # Show the downloaded image\n",
    "    plt.figure()\n",
    "    plt.imshow(np_image.astype(np.uint8))\n",
    "    plt.suptitle(\"Downloaded image\", fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Show the image that is actually being fed to the network\n",
    "    # The image was resized while preserving aspect ratio and then\n",
    "    # cropped. After that, the mean pixel value was subtracted from\n",
    "    # each pixel of that crop. We normalize the image to be between [-1, 1]\n",
    "    # to show the image.\n",
    "    plt.imshow( network_input / (network_input.max() - network_input.min()) )\n",
    "    plt.suptitle(\"Resized, Cropped and Mean-Centered input to the network\",\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    names = imagenet.create_readable_names_for_imagenet_labels()\n",
    "    for i in range(5):\n",
    "        index = sorted_inds[i]\n",
    "        # Now we print the top-5 predictions that the network gives us with\n",
    "        # corresponding probabilities. Pay attention that the index with\n",
    "        # class names is shifted by 1 -- this is because some networks\n",
    "        # were trained on 1000 classes and others on 1001. VGG-16 was trained\n",
    "        # on 1000 classes.\n",
    "        print('Probability %0.2f => [%s]' % (probabilities[index], names[index+1]))\n",
    "        \n",
    "    res = slim.get_model_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slim.get_model_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mobilenet_v1_0.50_128.ckpt.data-00000-of-00001\r\n",
      "mobilenet_v1_0.50_128.ckpt.index\r\n",
      "mobilenet_v1_0.50_128.ckpt.meta\r\n",
      "\u001b[0m\u001b[01;34mmobilenet_v1_0.50_128_2017_06_14\u001b[0m/\r\n",
      "mobilenet_v1_0.50_128_2017_06_14.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "ls /notebooks/test/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/notebooks/test/checkpoints/vgg_16.ckpt'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(checkpoints_dir, 'vgg_16.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slim.get_model_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First_Student_IC_school_bus_202076.jpg\r\n",
      "mobilenet_v1_0.50_128.ckpt\r\n",
      "mobilenet_v1_0.50_128.ckpt.data-00000-of-00001\r\n",
      "mobilenet_v1_0.50_128.ckpt.index\r\n",
      "mobilenet_v1_0.50_128.ckpt.meta\r\n",
      "\u001b[0m\u001b[01;34mmobilenet_v1_0.50_128_2017_06_14\u001b[0m/\r\n",
      "mobilenet_v1_0.50_128_2017_06_14.tar.gz\r\n",
      "vgg_16.ckpt\r\n",
      "vgg_16_2016_08_28.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "ls checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
